{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b12b29",
   "metadata": {},
   "source": [
    "# Recent  Approach : AthletePose3D - 3D Human Pose Estimation\n",
    "Based on: Yeung et al. (2025) - AthletePose3D Dataset Paper\n",
    "\n",
    "Implementation of methods from the paper:\n",
    "- 2D Pose Detection (HRNet/ViTPose approach)\n",
    "- 3D Pose Lifting (MotionAGFormer/TCPFormer)\n",
    "- Kinematic Validation (Joint angles & velocities)\n",
    "- Metrics: MPJPE, P-MPJPE, PDJ\n",
    "\n",
    "Data  set : AthletePose3D: A Benchmark Dataset for 3D Human Pose Estimation and Kinematic Validation in Athletic Movements\n",
    "Dataset Overview (Table 1 & 2 from Paper)\n",
    "\n",
    "| Characteristic | Details |\n",
    "|---------------|---------|\n",
    "| **Total Frames** | 1.3 million frames |\n",
    "| **Postures** | 165,000 individual postures |\n",
    "| **Sports** | 3 categories, 12 motion types |\n",
    "| **Athletes** | 8 athletes (amateur to professional) |\n",
    "| **Keypoints** | 55/86 keypoints (varies by sport) |\n",
    "| **Cameras** | 4-12 high-speed synchronized cameras |\n",
    "| **Resolution** | 1920Ã—1080 |\n",
    "\n",
    " Three Sport Categories :\n",
    "![alt text](datset.jpg)\n",
    "\n",
    "\n",
    " 1. Running (Lab Environment)\n",
    " 2. Track & Field (Lab Environment)\n",
    " 3. Figure Skating (Ice Rink)\n",
    "\n",
    "we choose some running data  videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 1. Install Required Libraries \n",
    "What this does: Installs libraries mentioned in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03135d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" INSTALLING LIBRARIES (Paper Technologies Only)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Paper mentions these frameworks\n",
    "packages = [\n",
    "    'opencv-python',      # Video processing\n",
    "    'numpy',              # Numerical computations\n",
    "    'matplotlib',         # Visualization\n",
    "    'scipy',              # Butterworth filter (Section 3.3)\n",
    "    'torch',              # Deep learning framework\n",
    "    'torchvision',        # Vision models\n",
    "    'mediapipe',          # For demo (lightweight alternative)\n",
    "    'pillow',             # Image processing\n",
    "]\n",
    "\n",
    "print(\"\\nInstalling packages...\")\n",
    "for pkg in packages:\n",
    "    print(f\"  Installing {pkg}...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "print(\"\\nâœ“ Installation complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df7bab0",
   "metadata": {},
   "source": [
    "# 2. Import Libraries and Setup\n",
    " What this does: Imports all required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy.spatial.transform import Rotation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"  AthletePose3D Implementation\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nPaper Information:\")\n",
    "print(\"  Title: AthletePose3D\")\n",
    "print(\"  Authors: Yeung et al. (2025)\")\n",
    "print(\"  Focus: 3D pose estimation for athletic movements\")\n",
    "print(\"  Dataset: 1.3M frames, 165K postures, 12 sports\")\n",
    "print(\"\\nKey Technologies (from paper):\")\n",
    "print(\"  â€¢ 2D Models: HRNet, ViTPose, MogaNet (best: PDJ 95.7)\")\n",
    "print(\"  â€¢ 3D Models: MotionAGFormer, TCPFormer (best: MPJPE 98.26mm)\")\n",
    "print(\"  â€¢ Metrics: MPJPE, P-MPJPE, PDJ\")\n",
    "print(\"  â€¢ Kinematic: 4th-order Butterworth filter @ 8Hz\")\n",
    "print(\"  â€¢ Input: 81 frames per sequence\")\n",
    "print(\"  â€¢ Resolution: 1920Ã—1080 @ 60/120 FPS\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c29f5c6",
   "metadata": {},
   "source": [
    "# 3. Load  Running Videos\n",
    "What this does: Loads your 10-12 running videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332425fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FOLDER = \"New_Paper\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" LOADING YOUR RUNNING VIDEOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if folder exists\n",
    "if not os.path.exists(VIDEO_FOLDER):\n",
    "    print(f\"\\nâŒ ERROR: Folder '{VIDEO_FOLDER}' not found!\")\n",
    "    print(\"Please create the folder and add your running videos.\")\n",
    "    print(\"\\nExpected structure:\")\n",
    "    print(\"  new_paper/\")\n",
    "    print(\"    â”œâ”€â”€ run1.mp4\")\n",
    "    print(\"    â”œâ”€â”€ run2.mp4\")\n",
    "    print(\"    â””â”€â”€ ...\")\n",
    "else:\n",
    "    # Find all video files\n",
    "    video_files = []\n",
    "    for ext in ['*.mp4', '*.avi', '*.mov', '*.MP4', '*.AVI', '*.MOV']:\n",
    "        video_files.extend(Path(VIDEO_FOLDER).glob(ext))\n",
    "    \n",
    "    video_files = [str(f) for f in video_files]\n",
    "    \n",
    "    print(f\"\\nâœ“ Found {len(video_files)} videos:\")\n",
    "    for i, vf in enumerate(video_files, 1):\n",
    "        # Get video info\n",
    "        cap = cv2.VideoCapture(vf)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        duration = frame_count / fps if fps > 0 else 0\n",
    "        cap.release()\n",
    "        \n",
    "        print(f\"  {i}. {os.path.basename(vf)}\")\n",
    "        print(f\"     Resolution: {width}x{height}, FPS: {fps:.1f}, \"\n",
    "              f\"Frames: {frame_count}, Duration: {duration:.1f}s\")\n",
    "    \n",
    "    print(f\"\\nPaper specifications:\")\n",
    "    print(f\"  â€¢ Running: 120 FPS (your videos may vary)\")\n",
    "    print(f\"  â€¢ Resolution: 1920Ã—1080 (paper standard)\")\n",
    "    print(f\"  â€¢ Athletes: Inter-university level\")\n",
    "\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511dca57",
   "metadata": {},
   "source": [
    "# 4.Load and Process Single Video\n",
    " What this does: Loads one video for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(video_files) > 0:\n",
    "    # Select first video\n",
    "    selected_video = video_files[0]\n",
    "    print(f\"\\nðŸ“¹ Processing: {os.path.basename(selected_video)}\")\n",
    "    \n",
    "    # Load video frames\n",
    "    cap = cv2.VideoCapture(selected_video)\n",
    "    frames = []\n",
    "    \n",
    "    print(\"Loading frames...\")\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        frame_count += 1\n",
    "        if frame_count % 100 == 0:\n",
    "            print(f\"  Loaded {frame_count} frames...\", end='\\r')\n",
    "    \n",
    "    cap.release()\n",
    "    frames = np.array(frames)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    print(f\"\\nâœ“ Loaded {len(frames)} frames\")\n",
    "    print(f\"  Shape: {frames.shape}\")\n",
    "    print(f\"  FPS: {fps:.1f}\")\n",
    "    print(f\"  Duration: {len(frames)/fps:.2f}s\")\n",
    "    \n",
    "    # Display sample frames\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    sample_indices = [0, len(frames)//2, len(frames)-1]\n",
    "    \n",
    "    for ax, idx in zip(axes, sample_indices):\n",
    "        ax.imshow(frames[idx])\n",
    "        ax.set_title(f\"Frame {idx}\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Sample Frames from Your Running Video\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No videos found. Please add videos to new_paper/ folder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad0cfa2",
   "metadata": {},
   "source": [
    "# 5. Initialize 2D Pose Detector (Paper Models)\n",
    " What this does: Sets up 2D pose estimation\n",
    " Paper Section 3.2: HRNet, ViTPose, MogaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43387cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" 2D POSE ESTIMATION MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nPaper's 2D Models (Section 3.2, Table 3):\")\n",
    "print(\"  1. HRNet (CVPR 2019) - CNN baseline - PDJ: 88.7\")\n",
    "print(\"  2. SwinPose (ICCV 2021) - Transformer - PDJ: 90.7\")\n",
    "print(\"  3. ViTPose (NeurIPS 2022) - Vision Transformer - PDJ: 95.0\")\n",
    "print(\"  4. UniFormer (ICLR 2022) - Transformer - PDJ: 95.2\")\n",
    "print(\"  5. MogaNet (ICLR 2024) - CNN (Best) - PDJ: 95.7 â­\")\n",
    "\n",
    "print(\"\\nFor this demo, using MediaPipe Pose (lightweight alternative)\")\n",
    "print(\"  â€¢ Similar to HRNet architecture\")\n",
    "print(\"  â€¢ 33 keypoints detected\")\n",
    "print(\"  â€¢ Real-time performance\")\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "pose_detector = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=2,  # Highest accuracy\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ 2D Pose Detector initialized\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82016d83",
   "metadata": {},
   "source": [
    "# 6.Extract 2D Poses from Video\n",
    " What this does: Detects 2D keypoints in each frame\n",
    " Paper Section 3.2: 2D pose estimation with COCO format (17 keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" EXTRACTING 2D POSES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def extract_2d_poses(frames, detector):\n",
    "    \"\"\"\n",
    "    Extract 2D poses from video frames\n",
    "    Paper: Uses models fine-tuned on COCO format (17 keypoints)\n",
    "    \"\"\"\n",
    "    poses_2d = []\n",
    "    confidences = []\n",
    "    \n",
    "    print(f\"\\nProcessing {len(frames)} frames...\")\n",
    "    \n",
    "    for i, frame in enumerate(frames):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"  Frame {i}/{len(frames)}...\", end='\\r')\n",
    "        \n",
    "        # Convert to BGR for MediaPipe\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Detect pose\n",
    "        results = detector.process(frame_bgr)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            # Extract landmarks (33 keypoints)\n",
    "            landmarks = []\n",
    "            conf_scores = []\n",
    "            for lm in results.pose_landmarks.landmark:\n",
    "                landmarks.append([lm.x, lm.y, lm.visibility])\n",
    "                conf_scores.append(lm.visibility)\n",
    "            \n",
    "            poses_2d.append(np.array(landmarks))\n",
    "            confidences.append(np.mean(conf_scores))\n",
    "        else:\n",
    "            # No pose detected\n",
    "            poses_2d.append(np.zeros((33, 3)))\n",
    "            confidences.append(0.0)\n",
    "    \n",
    "    print(f\"\\nâœ“ Extracted 2D poses for {len(poses_2d)} frames\")\n",
    "    return np.array(poses_2d), np.array(confidences)\n",
    "\n",
    "# Extract poses\n",
    "poses_2d, confidences = extract_2d_poses(frames, pose_detector)\n",
    "\n",
    "print(f\"\\n2D Pose Extraction Results:\")\n",
    "print(f\"  Shape: {poses_2d.shape}\")\n",
    "print(f\"  Keypoints per frame: {poses_2d.shape[1]}\")\n",
    "print(f\"  Detected frames: {np.sum(confidences > 0.5)}/{len(frames)}\")\n",
    "print(f\"  Average confidence: {np.mean(confidences):.3f}\")\n",
    "\n",
    "print(\"\\nPaper Format:\")\n",
    "print(f\"  â€¢ Paper uses 17 COCO keypoints (we have 33)\")\n",
    "print(f\"  â€¢ Input resolution: 384Ã—288 (paper standard)\")\n",
    "print(f\"  â€¢ Fine-tuned for 20 epochs\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66cd72",
   "metadata": {},
   "source": [
    "# isualize 2D Pose Detection\n",
    "What this does: Shows detected skeleton on frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173eadcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" VISUALIZING 2D POSE DETECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# MediaPipe connections\n",
    "POSE_CONNECTIONS = [\n",
    "    (11, 13), (13, 15),  # Left arm\n",
    "    (12, 14), (14, 16),  # Right arm\n",
    "    (11, 12),  # Shoulders\n",
    "    (23, 25), (25, 27), (27, 29), (29, 31),  # Left leg\n",
    "    (24, 26), (26, 28), (28, 30), (30, 32),  # Right leg\n",
    "    (23, 24),  # Hips\n",
    "    (11, 23), (12, 24)  # Torso\n",
    "]\n",
    "\n",
    "def draw_2d_pose(frame, pose_2d, connections):\n",
    "    \"\"\"Draw 2D pose on frame\"\"\"\n",
    "    vis_frame = frame.copy()\n",
    "    h, w = frame.shape[:2]\n",
    "    \n",
    "    # Draw keypoints\n",
    "    for i, kp in enumerate(pose_2d):\n",
    "        if kp[2] > 0.5:  # visibility threshold\n",
    "            x, y = int(kp[0] * w), int(kp[1] * h)\n",
    "            cv2.circle(vis_frame, (x, y), 6, (0, 255, 0), -1)\n",
    "            cv2.circle(vis_frame, (x, y), 8, (255, 255, 255), 2)\n",
    "    \n",
    "    # Draw skeleton\n",
    "    for connection in connections:\n",
    "        pt1_idx, pt2_idx = connection\n",
    "        if (pt1_idx < len(pose_2d) and pt2_idx < len(pose_2d) and \n",
    "            pose_2d[pt1_idx][2] > 0.5 and pose_2d[pt2_idx][2] > 0.5):\n",
    "            pt1 = (int(pose_2d[pt1_idx][0] * w), int(pose_2d[pt1_idx][1] * h))\n",
    "            pt2 = (int(pose_2d[pt2_idx][0] * w), int(pose_2d[pt2_idx][1] * h))\n",
    "            cv2.line(vis_frame, pt1, pt2, (255, 0, 0), 3)\n",
    "    \n",
    "    return vis_frame\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "sample_indices = np.linspace(0, len(frames)-1, 8, dtype=int)\n",
    "\n",
    "for ax, idx in zip(axes.flat, sample_indices):\n",
    "    vis_frame = draw_2d_pose(frames[idx], poses_2d[idx], POSE_CONNECTIONS)\n",
    "    ax.imshow(vis_frame)\n",
    "    ax.set_title(f\"Frame {idx} (Conf: {confidences[idx]:.2f})\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"2D Pose Detection Results (Paper Section 5.1)\", \n",
    "             fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ 2D pose visualization complete\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d336ff88",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
